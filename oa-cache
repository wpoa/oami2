#!/usr/bin/env python
# -*- coding: utf-8 -*-

from os import listdir, path, remove, rename
from sys import argv, stderr, stdout

import csv
# csv.field_size_limit must be reset according to
# <http://lethain.com/handling-very-large-csv-and-xml-files-in-python/>
csv.field_size_limit(999999999)

import errno
import gobject, pygst
pygst.require("0.10")

import gst
import progressbar

import mutagen.oggtheora

import pprint

from helpers import media

try:
    action = argv[1]
    target = argv[2]
except IndexError:
    stderr.write("""
oa-cache – Open Access Media Importer local operations

usage:  oa-cache clear-media [source] |
        oa-cache clear-metadata [source] |
        oa-cache convert-media [source] |
        oa-cache find-media [source] |
        oa-cache list-articles [source] |
        oa-cache stats [source]

""")
    exit(1)

try:
    assert(action in ['clear-media', 'clear-metadata', \
        'convert-media', 'find-media', 'list-articles', 'stats'])
except AssertionError:  # invalid action
    stderr.write('Unknown action “%s”.\n' % action)
    exit(2)

try:
    exec "from sources import %s as source_module" % target
except ImportError:  # invalid source
    stderr.write("Unknown source “%s”.\n" % target)
    exit(3)

import config

if action == 'clear-media':
    media_raw_directory = config.get_media_refined_source_path(target)
    listing = listdir(media_raw_directory)

    metadata_refined_directory = config.get_metadata_refined_source_path(target)
    download_cache_path = path.join(metadata_refined_directory, 'download_cache')
    remove(download_cache_path)

    for filename in listing:
        media_path = path.join(media_raw_directory, filename)
        stderr.write("Removing “%s” … " % media_path)
        remove(media_path)
        stderr.write("done.\n")

if action == 'clear-metadata':
    metadata_refined_directory = config.get_metadata_refined_source_path(target)
    fail_cache_path = path.join(metadata_refined_directory, 'fail_cache')
    success_cache_path = path.join(metadata_refined_directory, 'success_cache')
    for cache_path in [fail_cache_path, success_cache_path]:
        stderr.write("Removing “%s” … " % cache_path)
        try:
            remove(cache_path)
            stderr.write("done.\n")
        except OSError, e:
            stderr.write('\n%s\n' % str(e))

if action == 'convert-media':
    metadata_path = config.get_metadata_refined_source_path(target)
    converted_cache_path = path.join(metadata_path, 'converted_cache')
    download_cache_path = path.join(metadata_path, 'download_cache')
    with open(download_cache_path, 'r') as download_cache:
        with open(converted_cache_path, 'a') as converted_cache:
            reader = csv.reader(download_cache)
            writer = csv.writer(converted_cache)
            for row in reader:
                loop = gobject.MainLoop()

                media_refined_directory = config.get_media_refined_source_path(target)
                temporary_media_path = path.join(media_refined_directory, 'current.ogv')

                media_raw_path = row[14]
                filename = path.split(media_raw_path)[-1]
                media_refined_path = path.join(media_refined_directory, filename + '.ogv')

                if path.isfile(media_refined_path):
                    continue

                stderr.write("Converting “%s”, saving into “%s” …\n" % (
                        media_raw_path,
                        media_refined_path
                    )
                )

                m = media.Media(media_raw_path)
                m.find_streams()
                m.convert(temporary_media_path)

                try:
                    f = mutagen.oggtheora.OggTheora(temporary_media_path)
                    f['TITLE'] = row[9].decode('utf-8')
                    f['ALBUM'] = row[2].decode('utf-8')  # article title
                    f['ARTIST'] = row[1].decode('utf-8')  # authors
                    f['COPYRIGHTS'] = row[8].decode('utf-8')
                    f['LICENSE'] = row[7].decode('utf-8')
                    f['DESCRIPTION'] = row[10].decode('utf-8')
                    f['DATE'] = row[5].decode('utf-8')
                    f.save()
                except mutagen.oggtheora.OggTheoraHeaderError:
                    pass  # Most probably an encoding failure.

                rename(temporary_media_path, media_refined_path)

                row[14] = media_refined_path
                writer.writerow(row)

if action == 'list-articles':
    csv_writer = csv.writer(stdout)
    # categories based on:
    # “Citation Rules with Examples for Journal Articles on the Internet”
    # <http://www.ncbi.nlm.nih.gov/books/NBK7281/#A55596>
    csv_writer.writerow([
        'Authors',
        'Article Title',
        'Article Abstract',  # not part of citation rules, but useful
        'Journal Title',
        'Date of Publication',
        'Available from',
        'License',  # also not part of citation rules
        'Copyright Holder'  # same here
    ])
    source_path = config.get_metadata_raw_source_path(target)
    for result in source_module.list_articles(source_path):
        dataset = [item.encode('utf-8') for item in
            [
                result['article-contrib-authors'],
                result['article-title'],
                result['article-abstract'],
                result['journal-title'],
                result['article-date'],
                result['article-url'],
                result['article-license-url'],
                result['article-copyright-holder']
            ]
            if 'encode' in dir(item)
            # I have no idea why some results have no encode methods
        ]
        try:
            csv_writer.writerow(dataset)
        except IOError, e:
            if e.errno == errno.EPIPE:
                exit(0)  # broken pipe, exit normally
            else:
                raise

if action == 'find-media':
    results_directory = config.get_metadata_refined_source_path(target)

    fail_cache_path = path.join(results_directory, 'fail_cache')
    try:
        with open(fail_cache_path, 'r') as fail_cache:
            reader = csv.reader(fail_cache)
            fail_filenames = [row[0] for row in reader]
    except IOError:  # file does not exist on first run
        fail_filenames = []

    success_cache_path = path.join(results_directory, 'success_cache')
    try:
        with open(success_cache_path, 'r') as success_cache:
            reader = csv.reader(success_cache)
            success_filenames = [row[0] for row in reader]
    except IOError:  # file does not exist on first run
        success_filenames = []

    with open(fail_cache_path, 'a') as fail_cache:
        with open(success_cache_path, 'a') as success_cache:
            csv_writer_fail = csv.writer(fail_cache)
            csv_writer_success = csv.writer(success_cache)
            #csv_writer_success.writerow([
            #    'Name',
            #    'Authors',
            #    'Article Title',
            #    'Article Abstract',
            #    'Journal Title',
            #    'Date of Publication',
            #    'Available from',
            #    'License',
            #    'Copyright Holder',
            #    'Supplementary Material Label',
            #    'Supplementary Material Caption',
            #    'Supplementary Material Mimetype',
            #    'Supplementary Material Mime-Subtype',
            #    'Supplementary Material URL'
            #])
            source_path = config.get_metadata_raw_source_path(target)
            for result in source_module.list_articles(
                source_path,
                supplementary_materials=True,
                skip = fail_filenames + success_filenames
            ):
                materials = result['supplementary-materials']
                if materials:
                    stderr.write(
                        '%d supplementary materials in “%s”:\n\t' %
                        (
                            len(result['supplementary-materials']),
                            result['article-title'].encode('utf-8')
                        )
                    )
                    for material in materials:
                        stderr.write(
                            '%s/%s ' % (
                                material['mimetype'],
                                material['mime-subtype']
                            )
                        )
                        dataset = [item.encode('utf-8') for item in
                            [
                                result['name'],
                                result['article-contrib-authors'],
                                result['article-title'],
                                result['article-abstract'],
                                result['journal-title'],
                                result['article-date'],
                                result['article-url'],
                                result['article-license-url'],
                                result['article-copyright-holder'],
                                material['label'],
                                material['caption'],
                                material['mimetype'],
                                material['mime-subtype'],
                                material['url']
                            ]
                            if 'encode' in dir(item)
                            # I have no idea why some results have no encode methods
                        ]
                        csv_writer_success.writerow(dataset)
                    else:
                        csv_writer_fail.writerow([result['name']])
                    stderr.write('\n')

if action == "stats":
    results_directory = config.get_metadata_refined_source_path(target)
    success_cache_path = path.join(results_directory, 'success_cache')
    try:
        with open(success_cache_path, 'r') as success_cache:
            stderr.write('Counting records … ')
            reader = csv.reader(success_cache)
            total = sum(1 for row in reader)
            stderr.write(str(total) + ' records found.\n')
        with open(success_cache_path, 'r') as success_cache:
            reader = csv.reader(success_cache)
            p = progressbar.ProgressBar(maxval=total)
            completed = 0
            licenses = {
                'free': {},
                'non-free': {}
            }
            mimetypes = {
                'free': {},
                'non-free': {}
            }
            for row in reader:
                license_url = row[7]
                mimetype = row[11]
                mime_subtype = row[12]
                mimetype_composite = mimetype + '/' + mime_subtype
                if license_url in config.free_license_urls:
                    try:
                        licenses['free'][license_url] += 1
                    except KeyError:
                        licenses['free'][license_url] = 1
                    try:
                        mimetypes['free'][mimetype_composite] += 1
                    except KeyError:
                        mimetypes['free'][mimetype_composite] = 1
                else:
                    try:
                        licenses['non-free'][license_url] += 1
                    except KeyError:
                        licenses['non-free'][license_url] = 1
                    try:
                        mimetypes['non-free'][mimetype_composite] += 1
                    except KeyError:
                        mimetypes['non-free'][mimetype_composite] = 1
                completed += 1
                p.update(completed)

            stderr.write('\n')
            pp = pprint.PrettyPrinter(indent=4)
            output = pp.pformat({
                'licenses': licenses,
                'mimetypes': mimetypes
            })
            stdout.write(output + '\n')
    except IOError:  # file does not exist
        stderr.write('No records.')
